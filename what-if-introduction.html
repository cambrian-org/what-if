<section>
    <h2>Introduction</h2>
    <hr style="border-top: 1px solid #333; width: 100%; margin: 20px 0;">
    <p>

        The remarkable and sophisticated visual intelligence around us has very
        humble beginnings.
        The first light-detecting cells were likely simple photoreceptors,
        and the evolution of eyes has been a journey of increasing complexity
        against the backdrop of natural selection. But what if vision was only
        used for navigation or detection? What if eyes never evolved optical
        elements like lenses? What if animal brains stayed small throughout
        evolution? Answering these questions or testing these causal hypotheses
        would require us to re-run evolution. But What if there was
        a tool to instead simulate alternative paths that evolution didnâ€™t take?
        Let's try to build this tool.
    </p>

    <div class="evolution-row"
        style="justify-content: center; text-align: center;">
        <div class="video-container" style="margin: auto;">
            <img src="videos/precambrian.jpg"
                alt="Camera vs Compound"
                style="width: 100%; height: auto;">
            <div class="caption detection" style="text-align: center;">
                The painting, by G. Paselk, is an interpretation of the
                Precambrian mural at the Natural History Museum of the
                Smithsonian Institute.
            </div>
        </div>

        <img src="images/arrow.jpg" alt="Arrow"
            style="width: 5%; height: 100; margin: auto; display: block;">

        <div class="video-container" style="margin: auto;">
            <img src="videos/animal-eyes.jpg"
                alt="Camera vs Compound"
                style="width: 100%; height: auto;">
            <div class="caption detection" style="text-align: center;">
                Diveristy of animal eyes and animal visual intelligence.
                Each animal here has found a unique solution to the problem of
                visual intelligence: how to see and how to act in your
                environment.
            </div>
        </div>
    </div>

    <p>
        To ask what-if questions about vision evoltuion, we must model the
        evolutionary process in our embodied agents, allow for
        genetic mutation and selection through environment feedback. The digital
        anatomy of our agents allows for this.
        We mirror natural selection in the following way:
        an outer loop governs genetic inheritance and selection over
        evolutionary timescales, while an
        inner loop enables agents to learn via deep reinforcement learning
        through sensory feedback (lifetime adaptation). This nested
        structure reflects the Baldwin effect, where lifetime learning can guide
        and accelerate evolutionary adaptation. <em> The key here is
            that this allows the co-evolution of vision and behavior.</em>
    </p>

    <div class="evolution-row"
        style="justify-content: center; text-align: center;">
        <div class="video-container" style="margin: auto;">
            <img src="videos/genotype-page-001.jpg" alt="Genotype of an agent"
                style="width: 100%; height: auto;">
        </div>
    </div>
    <div class="caption detection" style="text-align: center;">
        The genetic encoding of the agent enables vision evolution. It mirrors
        the natural separation between sensory and neural development
        through three gene clusters with 10 <sup>20</sup> unique configurations
        in total. These are co-evolved based on direct feedback from the agent's
        behavior- similar to natural evolution.
    </div>

    <p>

        We create three tasks or games based on visual tasks that real animals
        face and had to overcome to survive. These are:
        <ul>
            <li>Orientation, Obstacle Avoidance and Navigation</li>
            <li>Food (Green) & Poison (Red) Discrimination (green and red are
                not visible to the agent)</li>
            <li>Prey (Green) & Predator (Red) Tracking (green and red are not
                visible to the agent)</li>
        </ul>

        In this way, each environment represents a single task which models the
        functional pressures hypothesized to garner the emergence of vision.
    </p>

    <div class="evolution-row">
        <div class="video-container">
            <video autoplay loop muted playsinline>
                <source src="videos/navigation.mp4" type="video/mp4">
            </video>
            <div class="caption navigation">Orientation & Navigation</div>
        </div>

        <div class="video-container">
            <video autoplay loop muted playsinline>
                <source src="videos/detection.mp4" type="video/mp4">
            </video>
            <div class="caption detection">Food & Poison Discrimination</div>
        </div>

        <div class="video-container">
            <video autoplay loop muted playsinline>
                <source src="videos/tracking.mp4" type="video/mp4">
            </video>
            <div class="caption tracking">Prey Tracking</div>
        </div>
    </div>

    Now let's use this computational framework to test hypotheses about how
    specific environmental pressures shape our emobided agent's eye
    morphologies, neural architectures, and behavior.

</section>
